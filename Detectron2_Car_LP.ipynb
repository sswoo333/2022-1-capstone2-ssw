{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sswoo333/2022-1-capstone2-ssw/blob/main/Detectron2_Car_LP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX0TMO3x0eVW"
      },
      "source": [
        "초기환경설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMcOwUCQ0jf4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDfSUwUH0jYV"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.1\n",
        "!pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#install old version of pytorch since detectron2 hasn't released packages for pytorch 1.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4UP25czVISE"
      },
      "outputs": [],
      "source": [
        " !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        " # After this setp it will ask you to restart the runtime, please do it "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJItbD7-VIvS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNxlkSXJ0K4k"
      },
      "source": [
        "utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7uw-xmO0KB3"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "import random \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#####\n",
        "from detectron2.engine import HookBase\n",
        "from detectron2.data import build_detection_train_loader\n",
        "import detectron2.utils.comm as comm\n",
        "#####\n",
        "\n",
        "def plot_samples(dataset_name, n=1):\n",
        "    dataset_custom = DatasetCatalog.get(dataset_name)\n",
        "    dataset_custom_metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "    for s in random.sample(dataset_custom, n):\n",
        "        img = cv2.imread(s[\"file_name\"])\n",
        "        v = Visualizer(img[:,:,::-1], metadata=dataset_custom_metadata, scale=1)\n",
        "        v = v.draw_dataset_dict(s)\n",
        "        plt.figure(figsize=(14,10))\n",
        "        plt.imshow(v.get_image())\n",
        "        plt.show()\n",
        "\n",
        "def get_train_cfg(config_file_path, checkpoint_url, train_datasetname, test_dataset_name, num_classes, device, output_dir):\n",
        "    cfg = get_cfg()\n",
        "\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(config_file_path))\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(checkpoint_url)\n",
        "    cfg.DATASETS.TRAIN = (train_dataset_name,)\n",
        "    cfg.DATASETS.TEST = (test_dataset_name,)  #  Validation \n",
        "\n",
        "    #####\n",
        "    cfg.DATASETS.VAL = (test_dataset_name,)\n",
        "    #####\n",
        "\n",
        "    \"\"\"cfg.TEST.EVAL_PERIOD = 100 # 1000 iteration 중에 100번마다 validation 수행 \"\"\"\n",
        "\n",
        "    cfg.DATALOADER.NUM_WORKERS = 1      # class의 개수 \n",
        "\n",
        "    cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "    cfg.SOLVER.BASE_LR = 0.00025    # Learning rate\n",
        "    cfg.SOLVER.MAX_ITER = 5000      # 학습횟수 iteration \n",
        "    cfg.SOLVER.STEPS = []\n",
        "\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
        "    cfg.MODEL.DEVICE = device\n",
        "    cfg.OUPUT_DIR = output_dir\n",
        "\n",
        "    return cfg\n",
        "\n",
        "#####\n",
        "\n",
        "\n",
        "class ValidationLoss(HookBase):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg.clone()\n",
        "        self.cfg.DATASETS.TRAIN = cfg.DATASETS.VAL\n",
        "        self._loader = iter(build_detection_train_loader(self.cfg))\n",
        "        \n",
        "    def after_step(self):\n",
        "        data = next(self._loader)\n",
        "        with torch.no_grad():\n",
        "            loss_dict = self.trainer.model(data)\n",
        "            \n",
        "            losses = sum(loss_dict.values())\n",
        "            assert torch.isfinite(losses).all(), loss_dict\n",
        "\n",
        "            loss_dict_reduced = {\"val_\" + k: v.item() for k, v in \n",
        "                                 comm.reduce_dict(loss_dict).items()}\n",
        "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "            if comm.is_main_process():\n",
        "                self.trainer.storage.put_scalars(total_val_loss=losses_reduced, \n",
        "                                                 **loss_dict_reduced)\n",
        "#####\n",
        "\n",
        "def on_image(image_path, predictor) :\n",
        "    im = cv2.imread(image_path)\n",
        "    outputs = predictor(im)\n",
        "    v = Visualizer(im[:,:,::-1], metadata={}, scale=1, instance_mode=ColorMode.SEGMENTATION)\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "    plt.figure(figsize=(14,10))\n",
        "    plt.imshow(v.get_image())\n",
        "    plt.show()\n",
        "\n",
        "def on_video(videoPath, predictor):\n",
        "    cap = cv2.VideoCapture(videoPath)\n",
        "    if (cap.isOpened()==False):\n",
        "        print(\"Error opening file....\")\n",
        "        return\n",
        "\n",
        "    (success,image) = cap.read()\n",
        "    while success:\n",
        "        predictions = predictor(image)\n",
        "        v = Visualizer(image[:,:,::-1], metadata={}, scale=1, instance_mode=ColorMode.SEGMENTATION)\n",
        "        output = v.draw_instance_predictions(predictions[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "        cv2.imshow(\"Result\", output.get_image()[:,:,::-1])\n",
        "\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord(\"q\"):\n",
        "            break\n",
        "        (success,image) = cap.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAbABcMz0Ozi"
      },
      "source": [
        "train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmk9w7lb0Pfj"
      },
      "outputs": [],
      "source": [
        "from detectron2.utils.logger import setup_logger\n",
        "\n",
        "setup_logger()\n",
        "\n",
        "from detectron2.data.datasets import register_coco_instances \n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "\n",
        "### 모델평가 관련\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "#from utils import *\n",
        "\n",
        "config_file_path = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\" #COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\n",
        "checkpoint_url = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "output_dir = \"./output/object_detection\" \n",
        "num_classes = 1 \n",
        "\n",
        "device = \"cuda\" # \"cuda\" or \"cpu\"\n",
        "\n",
        "train_dataset_name = \"LP_train\"\n",
        "train_images_path = \"train\"\n",
        "train_json_annot_path = \"train.json\"\n",
        "\n",
        "test_dataset_name = \"LP_test\"\n",
        "test_images_path = \"test\"\n",
        "test_json_annot_path = \"test.json\"\n",
        "\n",
        "cfg_save_path = \"OD_cfg.pickle\"\n",
        "\n",
        "##############################################\n",
        "\n",
        "if train_dataset_name in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(train_dataset_name)\n",
        "\n",
        "if test_dataset_name in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(test_dataset_name)\n",
        "\n",
        "##################################################\n",
        "\n",
        "register_coco_instances(name = train_dataset_name, metadata={},\n",
        "json_file= train_json_annot_path, image_root=train_images_path)\n",
        "\n",
        "register_coco_instances(name = test_dataset_name, metadata={},\n",
        "json_file= test_json_annot_path, image_root=test_images_path)\n",
        "\n",
        "plot_samples(dataset_name=train_dataset_name, n=2)\n",
        "\n",
        "################################################\n",
        "\n",
        "def main():\n",
        "    cfg = get_train_cfg(config_file_path, checkpoint_url, train_dataset_name, test_dataset_name, num_classes, device, output_dir)\n",
        "\n",
        "    with open(cfg_save_path, 'wb') as f :\n",
        "        pickle.dump(cfg,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    os.makedirs(cfg.OUTPUT_DIR,exist_ok=True)\n",
        "    trainer = DefaultTrainer(cfg)\n",
        "    #####\n",
        "    val_loss = ValidationLoss(cfg)  \n",
        "    trainer.register_hooks([val_loss])\n",
        "    # swap the order of PeriodicWriter and ValidationLoss\n",
        "    trainer._hooks = trainer._hooks[:-2] + trainer._hooks[-2:][::-1]\n",
        "    #####\n",
        "    \n",
        "    trainer.resume_or_load(resume=False)\n",
        "\n",
        "    trainer.train()  # 학습 시작 \n",
        "\n",
        "    ### 모델 평가(mAP)\n",
        "    evaluator = COCOEvaluator(\"LP_test\", cfg, False, output_dir=\"./output/\")\n",
        "    val_loader = build_detection_test_loader(cfg, \"LP_test\")\n",
        "    print(inference_on_dataset(trainer.model, val_loader, evaluator))  \n",
        "    ###\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "    main()\n",
        "    \n",
        "###################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaA15_UvY2vo"
      },
      "source": [
        "# **TensorBoard** Loss function graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeHNktf2Wsi4"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0EMSVfw0P6b"
      },
      "source": [
        "test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w5p5xBO0Qkq"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "#from utils import *\n",
        "\n",
        "cfg_save_path = \"OD_cfg.pickle\"\n",
        "\n",
        "with open(cfg_save_path,'rb') as f:\n",
        "    cfg = pickle.load(f)\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "image_path = \"test_L/Cars408.png\"\n",
        "#videoPath = \"test/highway.mp4\"\n",
        "\n",
        "on_image(image_path, predictor)\n",
        "#on_video(videoPath, predictor)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Detectron2 Car_LP.ipynb",
      "provenance": [],
      "mount_file_id": "18eM_ajzSIEXsj7FADSO_am8gyoakiHOP",
      "authorship_tag": "ABX9TyPiyiaxkg6KK4H9DWxoih0R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}